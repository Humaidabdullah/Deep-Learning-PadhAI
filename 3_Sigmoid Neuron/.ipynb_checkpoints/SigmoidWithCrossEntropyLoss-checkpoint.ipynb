{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPt7p1SNpf4kJLIBOyNfQOG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Humaidabdullah/Deep-Learning-PadhAI/blob/master/Sigmoid%20Neuron/SigmoidWithCrossEntropyLoss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhCnpZ_yz7ME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjvSLXxBYsTz",
        "colab_type": "text"
      },
      "source": [
        "Added Cross Entropy Loss function inside the class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LqkjKW80E0O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SigmoidNeuron:\n",
        "\n",
        "  def __init__(self):\n",
        "    self.w = None\n",
        "    self.b = None\n",
        "\n",
        "  def perceptron(self, x):\n",
        "    return np.dot(x,self.w.T) + self.b\n",
        "  \n",
        "  def sigmoid(self, x):\n",
        "    return 1.0/(1.0 + np.exp(-x))\n",
        "\n",
        "  def error(w,b):\n",
        "    #Cross Entropy Loss Function\n",
        "    err = 0.0\n",
        "    for x,y in zip(X,Y):\n",
        "      fx = self.sigmoid(self.perceptron(x))\n",
        "      err+ = -[(1-y)*math.log(1-fx,2) + y * math.log(fx,2)]\n",
        "    return err\n",
        "\n",
        "  def grad_w(self, x, y): \n",
        "    y_pred = self.sigmoid(self.perceptron(x))\n",
        "    return (y_pred - y) * x\n",
        "\n",
        "  def grad_b(self, x, y):\n",
        "    y_pred = self.sigmoid(self.perceptron(x))\n",
        "    return (y_pred - y)\n",
        "  \n",
        "  def fit(self, X, Y, epochs=1, lr = 1, initialize = True):\n",
        "    if initialize:\n",
        "      self.w = np.random.rand(1,X.shape[1]) #(row,col)\n",
        "      self.b = 0\n",
        "    for i in range(epochs):\n",
        "      dw = 0\n",
        "      db = 0\n",
        "      for x,y in zip(X,Y):\n",
        "        dw += self.grad_w(x,y)# This is actually an np array.\n",
        "        db += self.grad_b(x,y)\n",
        "      self.w -= lr * dw \n",
        "      self.b -= lr * db\n",
        "        \n",
        "  def predict(self, X):\n",
        "    Y_pred = []\n",
        "    for x in X:\n",
        "      y_pred = self.sigmoid(self.perceptron(x))\n",
        "      Y_pred.append(y_pred)\n",
        "    return np.array(Y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}